from langchain.chat_models import ChatOpenAI
from langchain.document_loaders import UnstructuredFileLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings
from langchain.vectorstores import FAISS
from langchain.storage import LocalFileStore
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough, RunnableLambda
from langchain.callbacks.base import BaseCallbackHandler
import streamlit as st
import os


# ë””ë ‰í† ë¦¬ ìƒì„± í•¨ìˆ˜
def create_directory(path):
    os.makedirs(path, exist_ok=True)


st.title("ğŸ¦œğŸ”— Streamlit is ğŸ”¥")

openai_api_key = st.sidebar.text_input("OpenAI API Key", type="password")
st.sidebar.markdown("https://github.com/Czerny40/gpt-challenge")

# ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state["messages"] = []  # ë©”ì‹œì§€ ê¸°ë¡
if "retriever" not in st.session_state:
    st.session_state.retriever = None


class ChatCallbackHandler(BaseCallbackHandler):
    def __init__(self):
        self.message = ""

    def on_llm_start(self, *args, **kwargs):
        self.message_box = st.empty()

    def on_llm_new_token(self, token: str, *args, **kwargs):
        self.message += token
        self.message_box.markdown(self.message)

    def on_llm_end(self, *args, **kwargs):
        save_message(self.message, "ai")


@st.cache_resource(show_spinner="íŒŒì¼ì„ ë¶„ì„í•˜ê³ ìˆì–´ìš”...")
def process_document(file):
    # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
    create_directory("./.cache/files")
    create_directory(f"./.cache/embeddings/{file.name}")

    file_content = file.read()
    file_path = f"./.cache/files/{file.name}"

    with open(file_path, "wb") as f:
        f.write(file_content)

    cache_dir = LocalFileStore(f"./.cache/embeddings/{file.name}")

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        separators=["\n\n", "\n"],
        length_function=len,
    )

    loader = UnstructuredFileLoader(file_path)
    docs = loader.load_and_split(text_splitter=splitter)

    embeddings = OpenAIEmbeddings(
        model="text-embedding-3-small", api_key=openai_api_key
    )

    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)

    vectorstore = FAISS.from_documents(docs, cached_embeddings)
    return vectorstore.as_retriever()


def get_response(message, retriever):
    llm = ChatOpenAI(
        model_name="gpt-4o-mini",
        temperature=0.1,
        api_key=openai_api_key,
    )

    chain = (
        {
            "context": retriever | RunnableLambda(lambda docs: format_documents(docs)),
            "question": RunnablePassthrough(),
        }
        | prompt
        | llm
    )

    return chain.invoke(message, config={"callbacks": [ChatCallbackHandler()]})


def send_message(message, role):
    with st.chat_message(role):
        st.markdown(message)
    save_message(message, role)


def save_message(message, role):
    st.session_state["messages"].append({"message": message, "role": role})


def load_chat_history():
    for message in st.session_state["messages"]:
        with st.chat_message(message["role"]):
            st.markdown(message["message"])


def format_documents(docs):
    return "\n\n".join(document.page_content for document in docs)


prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            """
        ë‹¹ì‹ ì€ ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ê·œì¹™ì„ ë”°ë¥´ì„¸ìš”:
        1. ì£¼ì–´ì§„ ë¬¸ì„œì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
        2. ì œê³µëœ ì •ë³´ê°€ ìˆë‹¤ë©´ ë°˜ë“œì‹œ ê·¸ ë‚´ìš©ì„ í¬í•¨í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
        3. ëª¨ë¥´ëŠ” ë‚´ìš©ì€ ì†”ì§íˆ ëª¨ë¥¸ë‹¤ê³  í•˜ì„¸ìš”

        ë¬¸ì„œ ë‚´ìš©:
        {context}
        """,
        ),
        ("human", "{question}"),
    ]
)

st.markdown(
    """
ì•ˆë…•í•˜ì„¸ìš”!

DocumentGPTëŠ” ì—…ë¡œë“œí•œ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•´ë“œë¦¬ëŠ” ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤!

ë¨¼ì € ì‚¬ì´ë“œë°”ì—ì„œ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”!
"""
)

st.sidebar.markdown(
    """
### ì§€ì›í•˜ëŠ” íŒŒì¼ í˜•ì‹
- PDF (.pdf)
- í…ìŠ¤íŠ¸ íŒŒì¼ (.txt)
- Word ë¬¸ì„œ (.docx)
"""
)

with st.sidebar:
    file = st.file_uploader(
        ".txt, .pdf, .docx íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”", type=["txt", "pdf", "docx"]
    )

if file:
    st.session_state.retriever = process_document(file)

    send_message("ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!", "ai", save=False)
    load_chat_history()

    message = st.chat_input("ì—…ë¡œë“œí•œ ë¬¸ì„œì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”")

    if message:
        send_message(message, "human")

        with st.chat_message("ai"):
            response = get_response(message, st.session_state.retriever)
            send_message(response.content, "ai")

else:
    if not openai_api_key:
        st.warning("ì‚¬ì´ë“œë°”ì— OpenAI API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”")
    st.session_state["messages"] = []
    st.session_state.retriever = None
