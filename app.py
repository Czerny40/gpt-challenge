from langchain.chat_models import ChatOpenAI
from langchain.document_loaders import UnstructuredFileLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings
from langchain.vectorstores import FAISS
from langchain.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from langchain.storage import LocalFileStore
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough, RunnableLambda
from langchain.callbacks.base import BaseCallbackHandler
import streamlit as st

st.title("ğŸ¦œğŸ”— Streamlit is ğŸ”¥")

openai_api_key = st.sidebar.text_input("OpenAI API Key", type="password")
st.sidebar.markdown("https://github.com/Czerny40/gpt-challenge")


class ChatCallbackHandler(BaseCallbackHandler):

    message = ""

    def on_llm_start(self, *args, **kwargs):
        self.message_box = st.empty()

    def on_llm_end(self, *args, **kwargs):
        save_message(self.message, "ai")

    def on_llm_new_token(self, token: str, *args, **kwargs):
        self.message += token
        self.message_box.markdown(self.message)


if "messages" not in st.session_state:
    st.session_state["messages"] = []


@st.cache_resource(show_spinner="íŒŒì¼ì„ ë¶„ì„í•˜ê³ ìˆì–´ìš”...")
def embed_file(file):

    file_name = file.name
    if "embeddings" not in st.session_state:
        st.session_state.embeddings = {}

    if file_name in st.session_state.embeddings:
        return st.session_state.embeddings[file_name]


    file_content = file.read()
    file_path = f"./.cache/files/{file.name}"
    with open(file_path, "wb") as f:
        f.write(file_content)

    # cache_dir = LocalFileStore(f"./.cache/embeddings/{file.name}")

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        separators=["\n\n", "\n"],
        length_function=len,
    )

    file_loader = UnstructuredFileLoader(file_path)
    docs = file_loader.load_and_split(text_splitter=splitter)

    embeddings = OpenAIEmbeddings(
        model="text-embedding-3-small", api_key=openai_api_key
    )

    # cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)

    vectorstore = FAISS.from_documents(docs, embeddings)

    retriever = vectorstore.as_retriever()

    # ì„¸ì…˜ì— ì„ë² ë”© ì €ì¥
    st.session_state.embeddings[file_name] = embeddings

    return retriever


# ë©”ì‹œì§€ í‘œì‹œ í•¨ìˆ˜
def send_message(message, role, save=True):
    with st.chat_message(role):
        st.markdown(message)
    if save:
        save_message(message, role)


# ë©”ì‹œì§€ ì €ì¥ í•¨ìˆ˜
def save_message(message, role):
    st.session_state["messages"].append({"message": message, "role": role})


# ì±„íŒ… ê¸°ë¡ ë¶ˆëŸ¬ì˜¤ê¸° í•¨ìˆ˜
def load_chat_history():
    for message in st.session_state["messages"]:
        send_message(message["message"], message["role"], save=False)


def format_documents(docs):
    return "\n\n".join(document.page_content for document in docs)


prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            """
        ë‹¹ì‹ ì€ ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ê·œì¹™ì„ ë”°ë¥´ì„¸ìš”:
        1. ì£¼ì–´ì§„ ë¬¸ì„œì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
        2. ì œê³µëœ ì •ë³´ê°€ ìˆë‹¤ë©´ ë°˜ë“œì‹œ ê·¸ ë‚´ìš©ì„ í¬í•¨í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
        3. ëª¨ë¥´ëŠ” ë‚´ìš©ì€ ì†”ì§íˆ ëª¨ë¥¸ë‹¤ê³  í•˜ì„¸ìš”

        ë¬¸ì„œ ë‚´ìš©:
        {context}
        """,
        ),
        ("human", "{question}"),
    ]
)

st.markdown(
    """
ì•ˆë…•í•˜ì„¸ìš”!

DocumentGPTëŠ” ì—…ë¡œë“œí•œ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•´ë“œë¦¬ëŠ” ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤!

ë¨¼ì € ì‚¬ì´ë“œë°”ì—ì„œ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”!
"""
)

st.sidebar.markdown(
    """
### ì§€ì›í•˜ëŠ” íŒŒì¼ í˜•ì‹
- PDF (.pdf)
- í…ìŠ¤íŠ¸ íŒŒì¼ (.txt)
- Word ë¬¸ì„œ (.docx)

"""
)

with st.sidebar:
    file = st.file_uploader(
        ".txt, .pdf, .docx íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”", type=["txt", "pdf", "docx"]
    )

if file and openai_api_key:
    retriever = embed_file(file)

    llm = ChatOpenAI(
        model_name="gpt-4o-mini",
        temperature=0.1,
        api_key=openai_api_key,
    )

    send_message("ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!", "ai", save=False)
    load_chat_history()
    message = st.chat_input("ì—…ë¡œë“œí•œ ë¬¸ì„œì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”")

    if message:
        send_message(message, "human")
        chain = (
            {
                "context": retriever
                | RunnableLambda(lambda docs: format_documents(docs)),
                "question": RunnablePassthrough(),
            }
            | prompt
            | llm
        )
        response = chain.invoke(message)
        send_message(response.content, "ai")
            

else:
    if not openai_api_key:
        st.warning("ì‚¬ì´ë“œë°”ì— OpenAI API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”")
    st.session_state["messages"] = []
