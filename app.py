from langchain.chat_models import ChatOpenAI
from langchain.document_loaders import UnstructuredFileLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings
from langchain.vectorstores import FAISS
from langchain.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from langchain.storage import LocalFileStore
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough, RunnableLambda
from langchain.callbacks.base import BaseCallbackHandler
import streamlit as st

st.title("ğŸ¦œğŸ”— Streamlit is ğŸ”¥")

openai_api_key = st.sidebar.text_input("OpenAI API Key", type="password")
st.sidebar.markdown("https://github.com/Czerny40/gpt-challenge")

# AI ì‘ë‹µì„ ì‹¤ì‹œê°„ìœ¼ë¡œ í‘œì‹œí•˜ê¸° ìœ„í•œ ì½œë°± í•¸ë“¤ëŸ¬
class ChatCallbackHandler(BaseCallbackHandler):

    message = ""

    def on_llm_start(self, *args, **kwargs):
        # AI ì‘ë‹µ ì‹œì‘ ì‹œ ë¹ˆ ë©”ì‹œì§€ ë°•ìŠ¤ ìƒì„±
        self.message_box = st.empty()

    def on_llm_end(self, *args, **kwargs):
        # AI ì‘ë‹µ ì™„ë£Œ ì‹œ ë©”ì‹œì§€ ì €ì¥
        save_message(self.message, "ai")

    def on_llm_new_token(self, token: str, *args, **kwargs):
        # ì‘ë‹µì´ í† í°ë¡œ ìƒì„±ë  ë•Œë§ˆë‹¤ ì‹¤ì‹œê°„ìœ¼ë¡œ í™”ë©´ì— í‘œì‹œ
        self.message += token
        self.message_box.markdown(self.message)


llm = ChatOpenAI(
    model_name="gpt-4o-mini",
    temperature=0.1,
    api_key=openai_api_key,
)
# ë©”ì‹œì§€ ì €ì¥ì†Œ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state["messages"] = []

# íŒŒì¼ ì„ë² ë”© í•¨ìˆ˜
@st.cache_resource(show_spinner="íŒŒì¼ì„ ë¶„ì„í•˜ê³ ìˆì–´ìš”...")
def embed_file(file):
    file_content = file.read()
    file_path = f"./.cache/files/{file.name}"
    with open(file_path, "wb") as f:
        f.write(file_content)

    cache_dir = LocalFileStore(f"./.cache/embeddings/{file.name}")

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        separators=["\n\n", "\n"],
        length_function=len,
    )

    file_loader = UnstructuredFileLoader(file_path)
    docs = file_loader.load_and_split(text_splitter=splitter)

    embeddings = OpenAIEmbeddings(
        model="text-embedding-3-small",
    )

    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)

    vectorstore = FAISS.from_documents(docs, cached_embeddings)

    vector_retriever = vectorstore.as_retriever(
        search_type="mmr", # MMR(Maximum Marginal Relevance) : ê²€ìƒ‰ ê²°ê³¼ì˜ í’ˆì§ˆê³¼ ë‹¤ì–‘ì„±ì„ ë™ì‹œì— ê³ ë ¤
        search_kwargs={
            "k": 4,  # ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜
            "fetch_k": 20,  # í›„ë³´ í’€ í¬ê¸°
            "lambda_mult": 0.7,  # ë‹¤ì–‘ì„± vs ê´€ë ¨ì„± ê°€ì¤‘ì¹˜ (1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ê´€ë ¨ì„± ì¤‘ì‹œ)
        },
    )

    # BM25 : ë¬¸ì„œ ê²€ìƒ‰ì„ ìœ„í•œ ê°•ë ¥í•œ ë­í‚¹ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ, ë¬¸ì„œì™€ ê²€ìƒ‰ì–´ ê°„ì˜ ê´€ë ¨ì„±ì„ í‰ê°€
    bm25_retriever = BM25Retriever.from_documents(docs)
    bm25_retriever.k = 4  # ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜

    # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ê¸° ìƒì„±
    ensemble_retriever = EnsembleRetriever(
        retrievers=[bm25_retriever, vector_retriever],
        weights=[0.5, 0.5],  # ê° ê²€ìƒ‰ê¸°ì˜ ê°€ì¤‘ì¹˜
    )

    return ensemble_retriever

# ë©”ì‹œì§€ í‘œì‹œ í•¨ìˆ˜
def send_message(message, role, save=True):
    with st.chat_message(role):
        st.markdown(message)
    if save:
        save_message(message, role)

# ë©”ì‹œì§€ ì €ì¥ í•¨ìˆ˜
def save_message(message, role):
    st.session_state["messages"].append({"message": message, "role": role})

# ì±„íŒ… ê¸°ë¡ ë¶ˆëŸ¬ì˜¤ê¸° í•¨ìˆ˜
def load_chat_history():
    for message in st.session_state["messages"]:
        send_message(message["message"], message["role"], save=False)


def format_documents(docs):
    return "\n\n".join(document.page_content for document in docs)


prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            """
        ë‹¹ì‹ ì€ ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ê·œì¹™ì„ ë”°ë¥´ì„¸ìš”:
        1. ì£¼ì–´ì§„ ë¬¸ì„œì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
        2. ì œê³µëœ ì •ë³´ê°€ ìˆë‹¤ë©´ ë°˜ë“œì‹œ ê·¸ ë‚´ìš©ì„ í¬í•¨í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
        3. ëª¨ë¥´ëŠ” ë‚´ìš©ì€ ì†”ì§íˆ ëª¨ë¥¸ë‹¤ê³  í•˜ì„¸ìš”

        ë¬¸ì„œ ë‚´ìš©:
        {context}
        """,
        ),
        ("human", "{question}"),
    ]
)

st.markdown(
    """
ì•ˆë…•í•˜ì„¸ìš”!

DocumentGPTëŠ” ì—…ë¡œë“œí•œ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•´ë“œë¦¬ëŠ” ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤!

ë¨¼ì € ì‚¬ì´ë“œë°”ì—ì„œ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”!
"""
)

st.sidebar.markdown(
    """
### ì§€ì›í•˜ëŠ” íŒŒì¼ í˜•ì‹
- PDF (.pdf)
- í…ìŠ¤íŠ¸ íŒŒì¼ (.txt)
- Word ë¬¸ì„œ (.docx)

"""
)

with st.sidebar:
    file = st.file_uploader(
        ".txt, .pdf, .docx íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”", type=["txt", "pdf", "docx"]
    )

if file:

    retriever = embed_file(file)

    send_message("ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!", "ai", save=False)
    load_chat_history()
    message = st.chat_input("ì—…ë¡œë“œí•œ ë¬¸ì„œì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”")

    if message:
        send_message(message, "human")
        chain = (
            {
                "context": retriever | RunnableLambda(format_documents),
                "question": RunnablePassthrough(),
            }
            | prompt
            | llm
        )
        with st.chat_message("ai"):
            response = chain.invoke(message)

else:
    st.session_state["messages"] = []
